{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import pandas as pd\n",
    "from irrCAC.table import CAC\n",
    "import warnings\n",
    "from IPython.display import Markdown, display\n",
    "import numpy as np\n",
    "\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "zip_path = \"./final annotations-20250313T022456Z-001.zip\"\n",
    "zf = zipfile.ZipFile(zip_path)\n",
    "\n",
    "\n",
    "list_ = []\n",
    "for file in zf.infolist():\n",
    "    if file.filename.endswith(\".xlsx\"):\n",
    "        df = pd.read_excel(zf.open(file.filename), skiprows=1, index_col=0)\n",
    "        list_.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "filenames = [\n",
    "    file.filename.replace(\"final annotations/\", \"\").replace(\".xlsx\", \"\")\n",
    "    for file in zf.infolist()\n",
    "    if file.filename.endswith(\".xlsx\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def progress_bar(percentage):\n",
    "    bar_length = 20  # Total length of the bar\n",
    "    filled_length = int(bar_length * (percentage / 100))\n",
    "    bar = \"[\" + \"#\" * filled_length + \"-\" * (bar_length - filled_length) + \"]\"\n",
    "    return bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "completed = []\n",
    "\n",
    "markdown_table = (\n",
    "    \"| i | spreadsheet | nans | progress | progress_bar |\\n|---|---|---|---|---|\\n\"\n",
    ")\n",
    "for i, (filename, dframe) in enumerate(zip(filenames, list_)):\n",
    "    nans = dframe.iloc[:, 2:-1].isna().sum().sum()\n",
    "    completed.append(840 - nans)\n",
    "    percentage = ((840 - nans) / 840) * 100  # Convert to percentage\n",
    "    markdown_table += f\"| {i} | {filename} | {nans} | {percentage:.1f}% | <progress id='file' max='100' value={percentage}></progress> |\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<label for=\"file\">Annotation progress 15.39%:</label>\n",
       "\n",
       "<progress id=\"file\" max=\"100\" value=\"15.39047619047619\"></progress>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "| i | spreadsheet | nans | progress | progress_bar |\n",
       "|---|---|---|---|---|\n",
       "| 0 | AI_debunkings20 | 800 | 4.8% | <progress id='file' max='100' value=4.761904761904762></progress> |\n",
       "| 1 | AI_debunkings | 840 | 0.0% | <progress id='file' max='100' value=0.0></progress> |\n",
       "| 2 | AI_debunkings8 | 805 | 4.2% | <progress id='file' max='100' value=4.166666666666666></progress> |\n",
       "| 3 | AI_debunkings16 | 840 | 0.0% | <progress id='file' max='100' value=0.0></progress> |\n",
       "| 4 | AI_debunkings9 | 840 | 0.0% | <progress id='file' max='100' value=0.0></progress> |\n",
       "| 5 | AI_debunkings21 | 527 | 37.3% | <progress id='file' max='100' value=37.26190476190476></progress> |\n",
       "| 6 | AI_debunkings4 | 714 | 15.0% | <progress id='file' max='100' value=15.0></progress> |\n",
       "| 7 | AI_debunkings7 | 840 | 0.0% | <progress id='file' max='100' value=0.0></progress> |\n",
       "| 8 | AI_debunkings19 | 840 | 0.0% | <progress id='file' max='100' value=0.0></progress> |\n",
       "| 9 | AI_debunkings14 | 840 | 0.0% | <progress id='file' max='100' value=0.0></progress> |\n",
       "| 10 | AI_debunkings22 | 840 | 0.0% | <progress id='file' max='100' value=0.0></progress> |\n",
       "| 11 | AI_debunkings6 | 805 | 4.2% | <progress id='file' max='100' value=4.166666666666666></progress> |\n",
       "| 12 | AI_debunkings5 | 840 | 0.0% | <progress id='file' max='100' value=0.0></progress> |\n",
       "| 13 | AI_debunkings0 | 770 | 8.3% | <progress id='file' max='100' value=8.333333333333332></progress> |\n",
       "| 14 | AI_debunkings1 | 512 | 39.0% | <progress id='file' max='100' value=39.04761904761905></progress> |\n",
       "| 15 | AI_debunkings18 | 840 | 0.0% | <progress id='file' max='100' value=0.0></progress> |\n",
       "| 16 | AI_debunkings3 | 0 | 100.0% | <progress id='file' max='100' value=100.0></progress> |\n",
       "| 17 | AI_debunkings23 | 2 | 99.8% | <progress id='file' max='100' value=99.76190476190476></progress> |\n",
       "| 18 | AI_debunkings10 | 427 | 49.2% | <progress id='file' max='100' value=49.166666666666664></progress> |\n",
       "| 19 | AI_debunkings13 | 840 | 0.0% | <progress id='file' max='100' value=0.0></progress> |\n",
       "| 20 | AI_debunkings17 | 793 | 5.6% | <progress id='file' max='100' value=5.595238095238096></progress> |\n",
       "| 21 | AI_debunkings12 | 728 | 13.3% | <progress id='file' max='100' value=13.333333333333334></progress> |\n",
       "| 22 | AI_debunkings11 | 840 | 0.0% | <progress id='file' max='100' value=0.0></progress> |\n",
       "| 23 | AI_debunkings2 | 840 | 0.0% | <progress id='file' max='100' value=0.0></progress> |\n",
       "| 24 | AI_debunkings24 | 805 | 4.2% | <progress id='file' max='100' value=4.166666666666666></progress> |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pgrs = sum(completed) / (840 * 25) * 100\n",
    "\n",
    "display(\n",
    "    Markdown(\n",
    "        f\"\"\"<label for=\"file\">Annotation progress {pgrs:.2f}%:</label>\n",
    "\n",
    "<progress id=\"file\" max=\"100\" value=\"{pgrs}\"></progress>\"\"\"\n",
    "    )\n",
    ")\n",
    "\n",
    "display(Markdown(markdown_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "model_map = pd.read_csv(\"ModelMap.csv\", index_col=0)\n",
    "ai_annotation = pd.read_excel(\n",
    "    \"/Users/francisco/Library/CloudStorage/OneDrive-Personal/Francisco/GitHub/eval-agent/app/automated_eval3.xlsx\",\n",
    "    index_col=0,\n",
    ")\n",
    "\n",
    "\n",
    "def human_ai_interrater_agreement_table(\n",
    "    human_annotation, column, ai_annotation=ai_annotation, model_map=model_map\n",
    "):\n",
    "\n",
    "    # Options map:\n",
    "    options_map = {\n",
    "        \"Model\": [],\n",
    "        \"claim\": [],\n",
    "        \"rebuttal\": [],\n",
    "        \"fact_1_relevance\": [\n",
    "            \"completely irrelevant\",\n",
    "            \"mostly irrelevant\",\n",
    "            \"mostly relevant\",\n",
    "            \"completely relevant\",\n",
    "        ],\n",
    "        \"fact_1_accuracy\": [\n",
    "            \"completely inaccurate\",\n",
    "            \"mostly inaccurate\",\n",
    "            \"mostly accurate\",\n",
    "            \"completely accurate\",\n",
    "        ],\n",
    "        \"familiarity\": [\n",
    "            \"completely unfamiliar\",\n",
    "            \"mostly unfamiliar\",\n",
    "            \"mostly familiar\",\n",
    "            \"completely familiar\",\n",
    "        ],\n",
    "        \"fact_2_relevance\": [\n",
    "            \"completely irrelevant\",\n",
    "            \"mostly irrelevant\",\n",
    "            \"mostly relevant\",\n",
    "            \"completely relevant\",\n",
    "        ],\n",
    "        \"fact_2_accuracy\": [\n",
    "            \"completely inaccurate\",\n",
    "            \"mostly inaccurate\",\n",
    "            \"mostly accurate\",\n",
    "            \"completely accurate\",\n",
    "        ],\n",
    "        \"fallacy_correctness\": [\n",
    "            \"completely incorrect\",\n",
    "            \"mostly incorrect\",\n",
    "            \"mostly correct\",\n",
    "            \"completely correct\",\n",
    "        ],\n",
    "        \"fallacy_clarity\": [\n",
    "            \"completely unclear\",\n",
    "            \"mostly unclear\",\n",
    "            \"mostly clear\",\n",
    "            \"completely clear\",\n",
    "        ],\n",
    "        \"comments\": [],\n",
    "    }\n",
    "\n",
    "    # 1. Add model key to annotation, and drop nan rows on \"Model\" column.\n",
    "    ann = pd.concat([model_map, human_annotation], axis=1).dropna(subset=[\"Model\"])\n",
    "    # rename columns:\n",
    "    ann.columns = list(options_map.keys())\n",
    "\n",
    "    # 2. Keep only the rows where the index exists in both DataFrames, normalize text\n",
    "    common_index = ann.index.intersection(ai_annotation.index)\n",
    "    ann = ann.loc[common_index, column].apply(lambda x: str(x).lower().strip())\n",
    "    ai = ai_annotation.loc[common_index, column].apply(lambda x: str(x).lower().strip())\n",
    "\n",
    "    # 4. create confusion matrix\n",
    "    init = pd.DataFrame(columns=[options_map[column]], index=[options_map[column]])\n",
    "    temp = pd.crosstab(ai, ann)\n",
    "\n",
    "    for idx in init.index:\n",
    "        for col in init.columns:\n",
    "            try:\n",
    "                init.loc[idx[0], col[0]] = temp.loc[idx[0], col[0]]\n",
    "            except KeyError:\n",
    "                init.loc[idx[0], col[0]] = 0\n",
    "\n",
    "    init = init.astype(float)\n",
    "\n",
    "    # 5. calculate scores\n",
    "\n",
    "    return init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "markdown_table = \"| i | percent | cohen | gwet AC1 |\\n\" \"|---|------|-------|------|\\n\"\n",
    "\n",
    "for i, ann in enumerate(list_):\n",
    "    cm = human_ai_interrater_agreement_table(ann, \"fact_1_relevance\")\n",
    "    table = CAC(cm)\n",
    "    percent = table.pa2()[\"est\"][\"coefficient_value\"]\n",
    "    cohen = table.cohen()[\"est\"][\"coefficient_value\"]\n",
    "    gwet = table.gwet()[\"est\"][\"coefficient_value\"]\n",
    "    if not np.isnan(cohen) or not np.isnan(gwet):\n",
    "        markdown_table += f\"| {i} | {percent:.2f} |{cohen:.2f} | {gwet:.2} |\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Inter-rater agreement table  \n",
       "### Ai-Human agreement table: Fact1 relevance"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "| i | percent | cohen | gwet AC1 |\n",
       "|---|------|-------|------|\n",
       "| 0 | 0.00 |0.00 | -0.2 |\n",
       "| 5 | 0.31 |0.03 | 0.16 |\n",
       "| 6 | 0.33 |-0.00 | 0.18 |\n",
       "| 13 | 0.33 |0.05 | 0.16 |\n",
       "| 14 | 0.31 |0.00 | 0.17 |\n",
       "| 16 | 0.34 |0.08 | 0.18 |\n",
       "| 17 | 0.27 |-0.05 | 0.096 |\n",
       "| 18 | 0.21 |-0.04 | -0.0078 |\n",
       "| 20 | 0.00 |-0.57 | -0.26 |\n",
       "| 21 | 0.30 |0.00 | 0.15 |\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(\n",
    "    Markdown(\n",
    "        \"\"\"## Inter-rater agreement table  \n",
    "### Ai-Human agreement table: Fact1 relevance\"\"\"\n",
    "    )\n",
    ")\n",
    "\n",
    "display(Markdown(markdown_table))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def human_human_interrater_agreement_table(\n",
    "    human_annotation_a, human_annotation_b, column, model_map=model_map\n",
    "):\n",
    "\n",
    "    # Options map:\n",
    "    options_map = {\n",
    "        \"Model\": [],\n",
    "        \"claim\": [],\n",
    "        \"rebuttal\": [],\n",
    "        \"fact_1_relevance\": [\n",
    "            \"completely irrelevant\",\n",
    "            \"mostly irrelevant\",\n",
    "            \"mostly relevant\",\n",
    "            \"completely relevant\",\n",
    "        ],\n",
    "        \"fact_1_accuracy\": [\n",
    "            \"completely inaccurate\",\n",
    "            \"mostly inaccurate\",\n",
    "            \"mostly accurate\",\n",
    "            \"completely accurate\",\n",
    "        ],\n",
    "        \"familiarity\": [\n",
    "            \"completely unfamiliar\",\n",
    "            \"mostly unfamiliar\",\n",
    "            \"mostly familiar\",\n",
    "            \"completely familiar\",\n",
    "        ],\n",
    "        \"fact_2_relevance\": [\n",
    "            \"completely irrelevant\",\n",
    "            \"mostly irrelevant\",\n",
    "            \"mostly relevant\",\n",
    "            \"completely relevant\",\n",
    "        ],\n",
    "        \"fact_2_accuracy\": [\n",
    "            \"completely inaccurate\",\n",
    "            \"mostly inaccurate\",\n",
    "            \"mostly accurate\",\n",
    "            \"completely accurate\",\n",
    "        ],\n",
    "        \"fallacy_correctness\": [\n",
    "            \"completely incorrect\",\n",
    "            \"mostly incorrect\",\n",
    "            \"mostly correct\",\n",
    "            \"completely correct\",\n",
    "        ],\n",
    "        \"fallacy_clarity\": [\n",
    "            \"completely unclear\",\n",
    "            \"mostly unclear\",\n",
    "            \"mostly clear\",\n",
    "            \"completely clear\",\n",
    "        ],\n",
    "        \"comments\": [],\n",
    "    }\n",
    "\n",
    "    # 1. Add model key to annotation, and drop nan rows on \"Model\" column.\n",
    "    annA = pd.concat([model_map, human_annotation_a], axis=1).dropna(subset=[\"Model\"])\n",
    "    annB = pd.concat([model_map, human_annotation_b], axis=1).dropna(subset=[\"Model\"])\n",
    "    # rename columns:\n",
    "    annA.columns = list(options_map.keys())\n",
    "    annB.columns = list(options_map.keys())\n",
    "\n",
    "    # 2. Keep only the rows where the index exists in both DataFrames, normalize text\n",
    "    common_index = annA.index.intersection(annB.index)\n",
    "    annA = annA.loc[common_index, column].apply(lambda x: str(x).lower().strip())\n",
    "    annB = annB.loc[common_index, column].apply(lambda x: str(x).lower().strip())\n",
    "\n",
    "    # 4. create confusion matrix\n",
    "    init = pd.DataFrame(columns=[options_map[column]], index=[options_map[column]])\n",
    "    temp = pd.crosstab(annA, annB)\n",
    "\n",
    "    for idx in init.index:\n",
    "        for col in init.columns:\n",
    "            try:\n",
    "                init.loc[idx[0], col[0]] = temp.loc[idx[0], col[0]]\n",
    "            except KeyError:\n",
    "                init.loc[idx[0], col[0]] = 0\n",
    "\n",
    "    init = init.astype(float)\n",
    "\n",
    "    # 5. calculate scores\n",
    "\n",
    "    return init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "init = pd.DataFrame(\n",
    "    columns=[f\"Ann{i}\" for i in range(len(list_))],\n",
    "    index=[f\"Ann{i}\" for i in range(len(list_))],\n",
    ")\n",
    "\n",
    "\n",
    "for i, ann_A in enumerate(list_):\n",
    "    for j, ann_B in enumerate(list_):\n",
    "        if i == j:\n",
    "            continue\n",
    "        cm = human_human_interrater_agreement_table(\n",
    "            ann_A, ann_B, \"fact_1_relevance\", model_map=model_map\n",
    "        )\n",
    "        table = CAC(cm)\n",
    "        init.iat[i, j] = table.gwet()[\"est\"][\"coefficient_value\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Inter-rater agreement table  \n",
       "### Gwet's AC1 scores: Fact1 relevance"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "|       | Ann0   | Ann5    | Ann6    | Ann13   | Ann14   | Ann16   | Ann17   | Ann18   | Ann20   | Ann21   |\n",
       "|:------|:-------|:--------|:--------|:--------|:--------|:--------|:--------|:--------|:--------|:--------|\n",
       "| Ann0  |        | 1.0     | 1.0     | 1.0     | 1.0     | 1.0     | 1.0     | 1.0     | -0.2    | 1.0     |\n",
       "| Ann5  | 1.0    |         | 0.76211 | 0.68889 | 0.94342 | 0.72226 | 0.75215 | 0.65926 | 0.20442 | 1.0     |\n",
       "| Ann6  | 1.0    | 0.76211 |         | 0.6152  | 0.80758 | 0.52818 | 0.63265 | 0.42246 | 0.17714 | 0.78723 |\n",
       "| Ann13 | 1.0    | 0.68889 | 0.6152  |         | 0.73034 | 0.47952 | 0.35867 | 0.33498 | 0.17714 | 0.63265 |\n",
       "| Ann14 | 1.0    | 0.94342 | 0.80758 | 0.73034 |         | 0.82422 | 0.79791 | 0.65949 | 0.20442 | 1.0     |\n",
       "| Ann16 | 1.0    | 0.72226 | 0.52818 | 0.47952 | 0.82422 |         | 0.71994 | 0.58884 | 0.17714 | 0.67093 |\n",
       "| Ann17 | 1.0    | 0.75215 | 0.63265 | 0.35867 | 0.79791 | 0.71994 |         | 0.54618 | 0.18644 | 0.78723 |\n",
       "| Ann18 | 1.0    | 0.65926 | 0.42246 | 0.33498 | 0.65949 | 0.58884 | 0.54618 |         | 0.16279 | 0.67093 |\n",
       "| Ann20 | -0.2   | 0.20442 | 0.17714 | 0.17714 | 0.20442 | 0.17714 | 0.18644 | 0.16279 |         | 0.20442 |\n",
       "| Ann21 | 1.0    | 1.0     | 0.78723 | 0.63265 | 1.0     | 0.67093 | 0.78723 | 0.67093 | 0.20442 |         |"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(\n",
    "    Markdown(\n",
    "        \"\"\"## Inter-rater agreement table  \n",
    "### Gwet's AC1 scores: Fact1 relevance\"\"\"\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "display(\n",
    "    Markdown(\n",
    "        init.dropna(axis=1, how=\"all\")\n",
    "        .dropna(axis=0, how=\"all\")\n",
    "        .fillna(\"\")\n",
    "        .to_markdown()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fact-checking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
